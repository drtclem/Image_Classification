{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification: cats & dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle imports up-front\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Import custom helper functions from utils.py\n",
    "from utils import single_training_run\n",
    "from utils import plot_single_training_run\n",
    "from utils import hyperparameter_optimization_run\n",
    "from utils import plot_hyperparameter_optimization_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### 1.1. Load the data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the training data\n",
    "training_data_path='../data/train'\n",
    "\n",
    "# Get a list of training dog and cat images\n",
    "training_dogs=glob.glob(f'{training_data_path}/dog/dog.*')\n",
    "training_cats=glob.glob(f'{training_data_path}/cat/cat.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the cat and dog images\n",
    "fig, axs = plt.subplots(3,2,figsize=(6, 4))\n",
    "\n",
    "for cat, dog, row in zip(training_cats, training_dogs, axs):\n",
    "    for animal, ax in zip([cat, dog], row):\n",
    "        animal=image.load_img(animal)\n",
    "        animal=image.img_to_array(animal)\n",
    "        animal/=255.0\n",
    "        ax.imshow(animal)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "\n",
    "Let's take a deeper look at a few of our images to get a feel for how image data is structured.\n",
    "\n",
    "### 2.1. Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one of the dogs\n",
    "dog=image.load_img(training_dogs[0])\n",
    "\n",
    "# And convert it to an array - this is how TensorFlow will handel the data\n",
    "dog=image.img_to_array(dog)\n",
    "\n",
    "# Take a look at some properties of the object\n",
    "print(f'Image data is: {type(dog)}')\n",
    "print(f'Image data shape: {dog.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has shape of 375 x 499 x 3? The image is 375 x 499 pixels, that makes sense. But what is the 3? Let's plot the pixel values and you will see what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dog[:,:,0].flatten(), bins=100, color='red', alpha=0.5, label='Red channel')\n",
    "plt.hist(dog[:,:,1].flatten(), bins=100, color='green', alpha=0.5, label='Green channel')\n",
    "plt.hist(dog[:,:,2].flatten(), bins=100, color='blue', alpha=0.5, label='Blue channel')\n",
    "plt.xlabel('Pixel value')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few interesting observations we can make here:\n",
    "\n",
    "1. This array has 3 axes: 375 x 499 x 3. The first two are the dimensions of the image, the third is the three color channels: red, green and blue.\n",
    "2. 375 x 499 x 3 is over a half million individual values - this one image is 10 time more data that any of the other datasets we have worked with so far!\n",
    "3. The range of pixel values is from 0 to about 250 - in reality it is (0,255) for a total range of 256 possible values per pixel. This is defined by the JPEG image standard.\n",
    "\n",
    "There are two things we can do with this information. First, we should scale the pixel values, this will improve the training of our neural network. We can do this directly in the model definition by adding a normalization layer. Second, we can make the images gray scale, which will decrease the input size and therefore computational burden by a factor of three. We can do this via the image dataset generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Image dimensions\n",
    "\n",
    "Let's take a look at a random sample of images from the dataset and see what their dimensions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample of images, half cats and half dogs\n",
    "sample_size=1000\n",
    "sample=random.sample(training_dogs, sample_size//2)\n",
    "sample+=random.sample(training_cats, sample_size//2)\n",
    "\n",
    "# Collectors for data\n",
    "heights=[]\n",
    "widths=[]\n",
    "\n",
    "# Loop on the sample images\n",
    "for sample_image in sample:\n",
    "\n",
    "    # Load the image and convert it to an array\n",
    "    sample_image=image.load_img(sample_image)\n",
    "    sample_image=image.img_to_array(sample_image)\n",
    "\n",
    "    # Get the width and height and add to collections\n",
    "    heights.append(sample_image.shape[0])\n",
    "    widths.append(sample_image.shape[1])\n",
    "\n",
    "# Plot results as a histogram\n",
    "plt.hist(heights, bins=50, alpha=0.5, label='Image heights')\n",
    "plt.hist(widths, bins=50, alpha=0.5, label='Image widths')\n",
    "plt.xlabel('Image dimension')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above plot, let's set our image dimension at 64 or 128 pixels. Smaller is better for training speed and memory use, but we don't want to go too small, then the model will have a hard time learning from the data. There is no hard and fast rule here. You could do an experiment testing several different image dimensions to see how small we could make the image and still get good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Image aspect ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sample image aspect ratios\n",
    "aspect_ratios=np.array(widths)/np.array(heights)\n",
    "\n",
    "# Plot as histogram\n",
    "plt.hist(aspect_ratios, bins=50, color='black')\n",
    "plt.xlabel('Image aspect ratio')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common aspect ratio in the data set looks to be around 1.3 - which is the '4:3' aspect ratio that used to be the standard for computer monitors. Rather than using square input images, we can use this aspect ratio to better match the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target aspect ratio. We will use this later to\n",
    "# calculate how tall the image should be based on the \n",
    "# requested image dim\n",
    "aspect_ratio=4/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "Now it's time to build and train the model. We will do this in a few steps, evaluating performance by looking at the training curves along the way:\n",
    "\n",
    "1. Establish baseline performance with default settings\n",
    "2. Optimize the batch size and learning rate\n",
    "3. Optimize regularization with L1 and L2 penalties\n",
    "4. Optimize input image size for speed and/or better performance\n",
    "5. Optimize network architecture\n",
    "6. Final model and evaluation\n",
    "\n",
    "### 2.1. Baseline model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Do a single training run with default settings\n",
    "training_results=single_training_run(training_data_path)\n",
    "\n",
    "# Collect the results so we can plot all of the experiments together at the end\n",
    "experiment_results={'Baseline model': training_results}\n",
    "\n",
    "# Plot the results\n",
    "plot_single_training_run(training_results).show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, not great. Does not seem like the model is learning at all. The accuracy curves are jumping all over the place. This likely indicates that the learning rate is too large and/or the batch size is too small. Let's optimize those hyperparameters first.\n",
    "\n",
    "### 2.2. Batch size and learning rate optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters={\n",
    "    'batch_sizes': [128, 256],\n",
    "    'learning_rates': [0.01, 0.0001]\n",
    "}\n",
    "\n",
    "# Train the model with each set of hyperparameters\n",
    "hyperparameter_optimization_results=hyperparameter_optimization_run(\n",
    "    training_data_path,\n",
    "    **hyperparameters\n",
    ")\n",
    "\n",
    "# Specify which hyperparameters to include in the plot labels\n",
    "plot_labels=['batch_sizes', 'learning_rates']\n",
    "\n",
    "# Plot the learning curves\n",
    "plot_hyperparameter_optimization_run(\n",
    "    hyperparameter_optimization_results,\n",
    "    hyperparameters,\n",
    "    plot_labels\n",
    ").show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training curves, it looks like the best results are obtained with a batch size of 256 and a learning rate of 0.0001. That conclusion is somewhat subjective and based on how close the training and validation score curves are and their smoothness. Running more steps per epoch will slow the training down, but may help smooth the curves out somewhat.\n",
    "\n",
    "### 2.3. Optimized model\n",
    "\n",
    "Let's use the best batch size and learning rate settings and train the models for longer to see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set some hyperparameters for the run\n",
    "hyperparameters={'batch_size': 256, 'learning_rate': 0.0001}\n",
    "\n",
    "# Do a single training run\n",
    "training_results=single_training_run(training_data_path, **hyperparameters)\n",
    "\n",
    "# Collect the results so we can plot all of the experiments together at the end\n",
    "experiment_results['Optimized model']=training_results\n",
    "\n",
    "# Plot the results\n",
    "plot_single_training_run(training_results).show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks better - but, we are clearly over fitting. Let's do a regularization experiment. We already built in the ability to pass L1 and L2 penalties to the dense layers, so here we go..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Regularization optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters={\n",
    "    'batch_sizes': [256],\n",
    "    'learning_rates': [0.0001],\n",
    "    'l1_penalties': [0.01, 0.001, 0.0001],\n",
    "    'l2_penalties': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Train the model with each combination of hyperparameters\n",
    "hyperparameter_optimization_results=hyperparameter_optimization_run(\n",
    "    training_data_path,\n",
    "    **hyperparameters\n",
    ")\n",
    "\n",
    "# Specify which hyperparameters to include in the plot labels\n",
    "plot_labels=['l1_penalties', 'l2_penalties']\n",
    "\n",
    "# Plot the training curves\n",
    "plot_hyperparameter_optimization_run(\n",
    "    hyperparameter_optimization_results,\n",
    "    hyperparameters,\n",
    "    plot_labels\n",
    ").show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves with and L1 penalty of 0.001 and an L2 penalty of 0.01 look best. This again is a little subjective; the goal if for the training and validation curves to be right on top of each other.\n",
    "\n",
    "### 2.5. Optimized & regularized model\n",
    "\n",
    "Now let's use the best values that we have determined for batch size, learning rate, L1 penalty and L2 penalty and train the model for longer to see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some hyperparameters for the run\n",
    "hyperparameters={\n",
    "    'l1_penalty': 0.001,\n",
    "    'l2_penalty': 0.01,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.0001\n",
    "}\n",
    "\n",
    "# Do a single training run\n",
    "training_results=single_training_run(training_data_path, **hyperparameters)\n",
    "\n",
    "# Collect the results so we can plot all of the experiments together at the end\n",
    "experiment_results['Regularized model']=training_results\n",
    "\n",
    "# Plot the results\n",
    "plot_single_training_run(training_results).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Much better. Now let's revisit the image size. So far we have been using gray scale images of 128 by 96 pixels. We may be able to:\n",
    "\n",
    "1. Use smaller images to create a more efficient model that maintains the same level of performance or\n",
    "2. Increase performance by using larger images\n",
    "\n",
    "### 2.6. Input image size optimization\n",
    "\n",
    "Try a few different input image sizes and see how the model does with the hyperparameter settings we have chosen via optimization so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters={\n",
    "    'batch_sizes': [256],\n",
    "    'learning_rates': [0.0001],\n",
    "    'l1_penalties': [0.001],\n",
    "    'l2_penalties': [0.01],\n",
    "    'image_widths': [32, 64, 128, 256]\n",
    "}\n",
    "\n",
    "# Train the model with each combination of hyperparameters\n",
    "hyperparameter_optimization_results=hyperparameter_optimization_run(\n",
    "    training_data_path,\n",
    "    **hyperparameters\n",
    ")\n",
    "\n",
    "# Specify which hyperparameters to include in the plot labels\n",
    "plot_labels=['image_widths']\n",
    "\n",
    "# Plot the training curves\n",
    "plot_hyperparameter_optimization_run(\n",
    "    hyperparameter_optimization_results,\n",
    "    hyperparameters,\n",
    "    plot_labels\n",
    ").show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some hyperparameters for the run\n",
    "hyperparameters={\n",
    "    'l1_penalty': 0.001,\n",
    "    'l2_penalty': 0.01,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.0001,\n",
    "    'image_width': 256,\n",
    "    'epochs': 500\n",
    "}\n",
    "\n",
    "# Do a single training run\n",
    "training_results=single_training_run(training_data_path, **hyperparameters)\n",
    "\n",
    "# Collect the results so we can plot all of the experiments together at the end\n",
    "experiment_results['Input size optimized model']=training_results\n",
    "\n",
    "# Plot the results\n",
    "plot_single_training_run(training_results).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
